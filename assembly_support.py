
import click
from csv import writer
from datetime import datetime
from os import listdir
from os.path import basename, isfile, join

# FastQ/BAM data manipulation
import pysam

# Pandas and numpy for data manipulation
import numpy as np
import pandas as pd


@click.group()
def main():
    pass


def logger(i):
    click.echo(f'{datetime.now()}: {i}', err=True)

"""
Support Functions for Ariadne
"""

def get_read_names(fq_file):
    logger('Importing read names from the file ' + fq_file)
    names = {}
    with open(fq_file, 'r') as fq:
        for i, line in enumerate(fq): 
            if (i % 4) == 0:
                full_name = line.strip()
                names[full_name.split(' ')[0]] = full_name
                if (i % 10000000) == 0:
                    logger('Processed ' + str(i) + ' lines')            
    logger('There are ' + str(i) + ' lines in total in ' + fq_file)
    return names


def get_read_full(fq_file):
    logger('Importing reads from the file ' + fq_file)
    reads = {}
    with open(fq_file, 'r') as fq:
        for i, line in enumerate(fq): 
            if (i % 4) == 0:
                curr_name = line.strip().split(' ')[0]
                reads[curr_name] = []
                if (i % 10000000) == 0:
                    logger('Processed ' + str(i) + ' lines')            
            reads[curr_name].append(line.strip())
    logger('There are ' + str(i) + ' lines in total in ' + fq_file)
    return reads 


@main.command('complete_reads')
@click.argument('original_fq') # FastQ file with original read clouds
@click.argument('enhanced_fq') # FastQ file with enhanced read clouds
@click.argument('full_fq') # Output FastQ file with missing unbarcoded reads
def complete_reads(original_fq, enhanced_fq, full_fq):
    """Adds reads missing from the Ariadne-generated enhanced FastQs based on the total set of reads in the original FastQs."""
    enh_names = get_read_names(enhanced_fq)
    all_reads = get_read_full(original_fq)
    num_processed = 0
    num_missing = 0
    with open(full_fq, 'w') as ff:
        for r in all_reads:
            r_info = all_reads[r]
            if r in enh_names:
                r_name = enh_names[r]
            else:
                r_name = r_info[0].replace('-1', '-0') # Indicates that the read was not deconvolved. 
                num_missing += 1
            ff.write(r_name + '\n' + r_info[1] + '\n' + r_info[2] + '\n' + r_info[3] + '\n')
            num_processed += 1
            if (num_processed % 1000000) == 0:
                logger('Processed ' + str(num_processed) + ' reads')
    logger('There are ' + str(num_missing) + ' reads from ' + original_fq + ' that were not included')


"""
Data Preprocessing for Other Tools
"""

def decimalToBinary(n):  
    return bin(int(n)).replace("0b", "")  


@main.command('bam_to_annotate')
@click.argument('bam') # Completed BAM file
@click.argument('id_csv') # Acccesion ID to reference sequence name  
def bam_to_annotate(bam, id_csv):
    """Generates read cloud enhancement information from BAM files. Output is [read, barcode (not in bwa files), direction, reference name]."""
    inbam = pysam.AlignmentFile(bam, 'rb')
    id2seq = {}
    id2seq['None'] = 'None'
    with open(id_csv, 'r') as ic:
        for line in ic:
            info = line.strip().split(',')
            id2seq[info[0]] = info[1]
    # Ingest BAM file and extract mapped-to references and read-directions in pairs. 
    with open(basename(bam).split('.')[0] + '.csv', 'w') as of:
        ow = writer(of)
        inbam = pysam.AlignmentFile(bam, 'rb')
        for read in inbam.fetch(until_eof=True):
            read_name = read.query_name
            read_dir = str(decimalToBinary(read.flag))[-7] # 0 = forward, 1 = reverse
            read_acc = read.reference_name if read.reference_name else 'None'
            if read.has_tag('BX'):
                read_barcode = read.get_tag('BX')[2:-2]
                ow.writerow([read_name, read_barcode, read_dir, id2seq[read_acc]])
            else:
                ow.writerow([read_name, read_dir, id2seq[read_acc]])


@main.command('add_barcodes')
@click.argument('fastq')
@click.argument('annot_csv')
def add_barcodes(fastq, annot_csv):
    """bwa-specific. Re-adds FastQ barcodes to bwa-based annotation file generated by bam_to_annotate. Output is [read, barcode, direction, reference name]."""
    read2bc = {}
    logger(f'Loading read cloud information from {fastq}')
    with open(fastq,'r') as fq:
        for i, line in enumerate(fq):
            if (i % 4) == 0:
                name_components = line.strip().split()
                if (len(name_components) > 1): # If the read has a barcode...
                    read2bc[name_components[0]] = name_components[1].split(':')[2].split('-')[0]
                else:
                    read2bc[name_components[0]] = 'None'
            if (i % 100000) == 0:
                logger(f'Finished processing {i} lines')
    df = pd.read_csv(annot_csv, names = ['Name', 'Direction', 'Reference'])
    df['Barcode']= df['Name'].map(read2bc)
    df = df[['Name', 'Barcode', 'Direction', 'Reference']]
    df.to_csv(basename(annot_csv).split('.')[0] + '.final.csv', index = False, header = False)


def split_into_chunks(l, n):
    for i in range(0, len(l), n):
        yield l[i:i + n]   # yields successive n-sized chunks of data


@main.command('subdiv_annotations')
@click.argument('infile') # Aggregated read information 
def subdiv_annotations(infile):
    """bwa-specific. Subsets total set of read clouds into smaller sets of read clouds (FastQ format still) so that the next step (concat_annotations) doesn't take forever."""
    logger(f'Started loading the annotations')
    df = pd.read_csv(infile, names = ['Name', 'Barcode', 'Direction', 'Reference'])
    logger(f'Finished loading the annotations')
    barcodes = df.Barcode.unique().tolist()
    barcodes.remove('None')
    logger(f'{len(barcodes)} read clouds')
    chunk_size = (int)(len(barcodes) / 100) + 1
    barcode_sublists = list(split_into_chunks(barcodes, chunk_size))
    logger(f'{len(barcode_sublists)} sublists of approximately {chunk_size} read clouds each')
    for i, bl in enumerate(barcode_sublists):
        df_b = df[df.Barcode.isin(bl)]
        df_b.to_csv(basename(infile).split('.')[0] + '.' + str(i) + '.csv', index = False, header = False)
        if (i % 10) == 0:
            logger(f'Finished processing {i} barcode sublists')


@main.command('concat_annotations')
@click.argument('infile') # Aggregated read information 
def concat_annotations(infile):
    """Aggregates reference information in each read cloud and converts that to enhanced groupings. Output is [read, enhanced_num]."""
    df = pd.read_csv(infile, names = ['Name', 'Barcode', 'Direction', 'Reference'])
    logger(f'Finished loading the annotations')
    barcodes = df.Barcode.unique()
    num_bc_processed = 0
    prefix = '.'.join(basename(infile).split('.')[:-1])
    logger(f'{len(barcodes)} read clouds')
    with open(prefix + '.R1.csv', 'w') as ff:
        with open(prefix + '.R2.csv', 'w') as rf:
            for b in barcodes:
                df_b = df.loc[df['Barcode'] == b]
                references = df_b.Reference.unique().tolist()
                if len(references) > 1:
                    for i in range(len(df_b)):
                        read_ref = references.index(df_b.iloc[i,3]) # Index/enhanced cloud of the read's reference
                        if df_b.iloc[i,2] == 0: 
                            ff.write(df_b.iloc[i,0] + ',' + str(read_ref) + '\n')
                        else:
                            rf.write(df_b.iloc[i,0] + ',' + str(read_ref) + '\n')
                num_bc_processed += 1
                if (num_bc_processed % 5000) == 0:
                    logger(f'{infile} {num_bc_processed}')


def load_from_dir(enh_dir):
    name_enh_dict = {}
    enh_list = [f for f in listdir(enh_dir) if isfile(join(enh_dir, f))]
    for i, f in enumerate(enh_list):
        name_enh_dict.update(load_from_csv(join(enh_dir,f)))
        if (i % 10 == 0):
            logger(f'Finished processing {i} enhanced CSVs')            
    return name_enh_dict     


def load_from_csv(enh_csv):
    name_enh_dict = {}
    with open(enh_csv,'r') as ef:
        for line in ef:
            read_info = line.strip().split(',')
            name_enh_dict[read_info[0]] = read_info[1]
    return name_enh_dict


@main.command('fastq_enhance')
@click.argument('in_fq')
@click.argument('outdir')
@click.option('--enh_dir', '-d', help='Read name-to-enhanced cloud number directory') 
@click.option('--enh_csv', '-c', help='Read name-to-enhanced cloud number CSV') 
def fastq_enhance(in_fq, enh_dir, enh_csv, outdir):
    """Replaces original cloud number with tool-enhanced number."""
    if enh_dir: 
        enhanced_dict = load_from_dir(enh_dir) # For bwa, where replacements are spread over many CSVs
    else:
        enhanced_dict = load_from_csv(enh_csv) # For ema, which produces a single CSV

    # Replace the original groupings with enhanced groupings
    in_fq_parts = basename(in_fq).split('.')
    out_fq = outdir + '/' + in_fq_parts[0] + '_bwa.' + in_fq_parts[1] + '.' + in_fq_parts[2]
    with open(out_fq, 'w') as of:
        with open(in_fq, 'r') as fq: 
            for i, line in enumerate(fq):
                if (i % 4) == 0: # Read name
                    out_read_name = line.strip()
                    name_components = out_read_name.split() 
                    if len(name_components) > 1: # If read is i) in a cloud, replace it with enhanced cloud numbers
                        if name_components[0][1:] in enhanced_dict: # Get rid of this post-testing
                            enh_num = enhanced_dict[name_components[0][1:]] # Enhanced cloud numbers
                            out_read_name = out_read_name[:-1] + str(enh_num) # Tag it on
                    of.write(out_read_name + '\n')
                else:
                    of.write(line)
                if (i % 10000000 == 0):
                    logger(f'Finished processing {i} lines from {in_fq}')            


if __name__ == '__main__':
    main()


